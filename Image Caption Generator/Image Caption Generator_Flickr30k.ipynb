{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_list = os.listdir('D:\\\\Projects\\\\Image Caption Generator\\\\flickr30k_images\\\\flickr30k_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30\n",
    "train_images_list = train_images_list[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (256, 256)\n",
    "num_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array([None] * sample_size)\n",
    "real_images = np.array([None] * sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in train_images_list:\n",
    "    real_images[j] = np.array(plt.imread('D:\\\\Projects\\\\Image Caption Generator\\\\flickr30k_images\\\\flickr30k_images\\\\' + i))\n",
    "    train[j] = np.array(plt.imread('D:\\\\Projects\\\\Image Caption Generator\\\\flickr30k_images\\\\flickr30k_images\\\\' + i))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in train:\n",
    "    train[j] = cv2.resize(i, size)\n",
    "    train[j] = train[j].reshape(1, size[0], size[1], num_channels)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.vstack(train[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(train[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = pd.read_csv('D:\\\\Projects\\\\Image Caption Generator\\\\flickr30k_images\\\\results.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_id(names):\n",
    "    names = [int(x.split('_')[-1].split('.')[0]) for x in names]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = get_images_id(train_images_list[:sample_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions.columns = ['image_name', 'comment_number', 'comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_map_caption(train_images_list, train_captions):\n",
    "    caption = []\n",
    "    for i in train_images_list:\n",
    "        caption.append(train_captions[train_captions['image_name'] == i]['comment'].iat[0])\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = np.array(images_map_caption(train_images_list, train_captions))\n",
    "print(captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tag = '<s>'\n",
    "end_tag = '<e>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(captions):\n",
    "    arr = []\n",
    "    m = captions.shape[0]\n",
    "    sentence = [None ] * m\n",
    "    j  = 0\n",
    "    for i in captions:\n",
    "        i = re.sub(' +',' ',i)\n",
    "        i = start_tag + ' ' + i + ' ' + end_tag\n",
    "        sentence[j] = i.split()\n",
    "        j += 1\n",
    "        arr = arr + i.split()\n",
    "    arr = list(set(arr))\n",
    "    vocab_size = len(arr)\n",
    "    j = 0\n",
    "    fwd_dict = {}\n",
    "    rev_dict = {}\n",
    "    j = 0\n",
    "    for i in arr:\n",
    "        fwd_dict[i] = j\n",
    "        rev_dict[j] = i\n",
    "        j += 1\n",
    "    return vocab_size, sentence, fwd_dict, rev_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, sentences, fwd_dict, rev_dict = get_vocab(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(sentences)\n",
    "train_caption = [None] * m\n",
    "i = 0\n",
    "for sentence in sentences:\n",
    "    cap_array = None\n",
    "    for word in sentence:\n",
    "        row = [0]\n",
    "        col = [fwd_dict[word]]\n",
    "        data = [1]\n",
    "        if cap_array is None:\n",
    "            cap_array = csr_matrix((data, (row, col)), shape=(1, vocab_size))\n",
    "        else:\n",
    "            cap_array = vstack((cap_array, csr_matrix((data, (row, col)), shape=(1, vocab_size))))\n",
    "    train_caption[i] = cap_array\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_caption[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Design ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(shape, suffix):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.7), name='W_' + suffix)\n",
    "\n",
    "def create_biases(size, suffix):\n",
    "    return tf.Variable(tf.zeros([size]), name='b_' + suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inp, kernel_shape, num_channels, num_kernels, suffix):\n",
    "    filter_shape = [kernel_shape[0], kernel_shape[1], num_channels, num_kernels]\n",
    "    weights = create_weights(shape=filter_shape, suffix=suffix)\n",
    "    biases = create_biases(num_kernels, suffix=suffix)\n",
    "    layer = tf.nn.conv2d(input=inp, filter=weights, padding='SAME', strides=[1, 1, 1, 1], name='conv_' + suffix)\n",
    "    layer += biases\n",
    "    layer = tf.nn.relu6(layer, name='relu_' + suffix)\n",
    "    #layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer, suffix):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features], name='flat_' + suffix )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inp, num_inputs, num_outputs, suffix, use_relu=True):\n",
    "    weights = create_weights([num_inputs, num_outputs], suffix)\n",
    "    biases = create_biases(num_outputs, suffix)\n",
    "    layer = tf.matmul(inp, weights) + biases\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell(Win ,Wout, Wfwd, b, hprev, inp):\n",
    "    h = tf.tanh(tf.add(tf.add(tf.matmul(inp, Win), tf.matmul(hprev, Wfwd)), b))\n",
    "    out = tf.matmul(h, Wo)\n",
    "    return h, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.device(\"/device:GPU:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "training_iters = 5000\n",
    "display_step = 1000\n",
    "max_sent_limit = 50\n",
    "num_tests = 12\n",
    "bridge_size = 1024\n",
    "keep_prob = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_caption = tf.placeholder(tf.float32, [None, vocab_size], name = 'x_caption')\n",
    "x_inp = tf.placeholder(tf.float32, shape=[1, size[0],size[1],num_channels], name='x_image')\n",
    "y = tf.placeholder(tf.float32, [None, vocab_size], name = 'x_caption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wconv = tf.Variable(tf.truncated_normal([bridge_size, vocab_size], stddev=0.7))\n",
    "bconv = tf.Variable(tf.zeros([1, vocab_size]))\n",
    "Wi= tf.Variable(tf.truncated_normal([vocab_size, vocab_size], stddev=0.7))\n",
    "Wf= tf.Variable(tf.truncated_normal([vocab_size, vocab_size], stddev=0.7))\n",
    "Wo= tf.Variable(tf.truncated_normal([vocab_size, vocab_size], stddev=0.7))\n",
    "b = tf.Variable(tf.zeros([1, vocab_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1 = conv_layer(inp=x_inp, kernel_shape=(3, 3), num_kernels=32, num_channels=3, suffix='1')\n",
    "layer_conv2 = conv_layer(inp=layer_conv1, kernel_shape=(3, 3), num_kernels=32, num_channels=32, suffix='2')\n",
    "maxpool1 = tf.nn.max_pool(layer_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\n",
    "layer_conv3 = conv_layer(inp=maxpool1, kernel_shape=(3, 3), num_kernels=64, num_channels=32, suffix='3')\n",
    "layer_conv4 = conv_layer(inp=layer_conv3, kernel_shape=(3, 3), num_kernels=64, num_channels=64, suffix='4')\n",
    "maxpool2 = tf.nn.max_pool(layer_conv4, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\n",
    "layer_conv5 = conv_layer(inp=maxpool2, kernel_shape=(3, 3), num_kernels=128, num_channels=64, suffix='5')\n",
    "layer_conv6 = conv_layer(inp=layer_conv5, kernel_shape=(3, 3), num_kernels=128, num_channels=128, suffix='6')\n",
    "maxpool3 = tf.nn.max_pool(layer_conv6, ksize=[1, 2, 2, 1], strides=[1, 2, 2,1], padding= 'SAME')\n",
    "layer_conv7 = conv_layer(inp=maxpool3, kernel_shape=(3, 3), num_kernels=256, num_channels=128, suffix='7')\n",
    "layer_conv8 = conv_layer(inp=layer_conv7, kernel_shape=(3, 3), num_kernels=256, num_channels=256, suffix='8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_layer = flatten_layer(layer_conv8, suffix='9')\n",
    "#flat_layer = tf.layers.dropout(flat_layer, rate= keep_prob)\n",
    "dense_layer_1 = dense_layer(inp=flat_layer, num_inputs=262144 , num_outputs=bridge_size, suffix='10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hook = tf.cast(csr_matrix(([1], ([0], [fwd_dict[start_tag]])), shape=(1, vocab_size)).A, tf.float32)\n",
    "end_hook = tf.cast(csr_matrix(([1], ([0], [fwd_dict[end_tag]])), shape=(1, vocab_size)).A, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = tf.slice(x_caption, [0, 0], [1, vocab_size])\n",
    "h = dense_layer_1\n",
    "h, out = rnn_cell(Wi ,Wo, Wconv, bconv, h, hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(prev, curr):\n",
    "    h = prev[0]\n",
    "    curr = tf.reshape(curr, [1, vocab_size])\n",
    "    h, out = rnn_cell(Wi ,Wo, Wf, b, h, curr)\n",
    "    return h, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, output = tf.scan(fn, x_caption[1:], initializer=(h, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.squeeze(output, axis  = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.concat([out, output], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.nn.softmax(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tensor = tf.TensorArray(dtype=tf.float32, dynamic_size=True, size = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htest = dense_layer_1\n",
    "htest, out_first = rnn_cell(Wi ,Wo, Wconv, bconv, htest, start_hook)\n",
    "t = 0\n",
    "out_ = tf.one_hot(tf.argmax(tf.nn.softmax(out_first), 1), depth=vocab_size)\n",
    "out_tensor = out_tensor.write(t, out_)\n",
    "t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(res, h, out_tensor, t):\n",
    "    return tf.logical_and(tf.logical_not(tf.equal(tf.argmax(res, 1)[0], fwd_dict[end_tag])), tf.less(t, max_sent_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(res, h, out_tensor, t):\n",
    "    h, out = rnn_cell(Wi ,Wo, Wf, b, h, res)\n",
    "    res = tf.one_hot(tf.argmax(tf.nn.softmax(out), 1), depth=vocab_size)\n",
    "    out_tensor = out_tensor.write(t, res)\n",
    "    return res, h, out_tensor, t + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, __, final_outputs, T = tf.while_loop(condition, action, [out_, htest, out_tensor, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = tf.squeeze(final_outputs.stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    m = len(train_caption)\n",
    "    for epoch in range(training_iters):\n",
    "        total_cost = 0\n",
    "        total_acc = 0\n",
    "        for i in range(m):\n",
    "            _, cst, acc = sess.run([optimizer, cost, accuracy], feed_dict = {x_caption:train_caption[i][:-1].A, x_inp:train[i:i+1], y:train_caption[i][1:].A})\n",
    "            total_cost += cst\n",
    "            total_acc += acc\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            print('After ', (epoch + 1), 'iterations: Cost = ', total_cost / m, 'and Accuracy: ', total_acc * 100/ m , '%' )\n",
    "    print('Optimization finished!')\n",
    "    print(\"Let's check\")\n",
    "    for tests in range(num_tests):\n",
    "        image_num = random.randint(0, sample_size - 1)\n",
    "        caption = sess.run(final_prediction, feed_dict = {x_inp:train[image_num:image_num + 1]})\n",
    "        print(caption.shape)\n",
    "        caption = np.argmax(caption[:-1], 1)\n",
    "        capt = ''\n",
    "        for i in caption:\n",
    "            capt += rev_dict[i] + ' '\n",
    "        print('Predicted Caption:->', capt)\n",
    "        orig_cap = np.argmax(train_caption[image_num:image_num + 1][0][1:-1].A, 1)\n",
    "        orignalcaption = ''\n",
    "        for i in orig_cap:\n",
    "            orignalcaption += rev_dict[i] + ' '\n",
    "        print('Orignal Caption:->', orignalcaption)\n",
    "        plt.imshow(real_images[image_num])\n",
    "        plt.title('Image')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
